{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification metrics\n",
    "\n",
    "Choosing right evaluation metrics for the problem is one of the most important aspect of machine learning. Choice of metrics allows us to compare performance of different models and helps in model selection.\n",
    "\n",
    "In this task, we will explore following metrics:\n",
    "- confusion matrix\n",
    "- accuracy\n",
    "- precision\n",
    "- recall\n",
    "- f1 score\n",
    "\n",
    "#### Dataset\n",
    "The training dataset is available at \"data/ozone_levels_train.csv\" in the respective challenge' repo.<br>\n",
    "The testing dataset is available at \"data/ozone_levels_test.csv\" in the respective challenge' repo.<br>\n",
    "\n",
    "The dataset is __modified version__ of the dataset 'ozone level' on provided by UCI Machine Learning repository.\n",
    "\n",
    "Original dataset: https://archive.ics.uci.edu/ml/datasets/Ozone+Level+Detection\n",
    "\n",
    "#### Objective\n",
    "To learn about classification metrics and compare logistic regression and decision tree on the same dataset\n",
    "\n",
    "#### Tasks\n",
    "- define X(input) and Y(output)\n",
    "- train the decision tree model \n",
    "- train the logistic model\n",
    "- construct a confusion matrix\n",
    "- calculate the classification accurace\n",
    "- calculate the Precision\n",
    "- calculate the Recall\n",
    "- calculate the F1 score\n",
    "- calculate Area Under ROC Curve\n",
    "\n",
    "#### Further fun\n",
    "- Calculate precission and recall\n",
    "- find the area under the curve for Roc metrics\n",
    "- impliment below metrics using inbuilt librarires\n",
    "        confusion matrix\n",
    "        accuracy\n",
    "        precision\n",
    "        recall\n",
    "        f1 score\n",
    "\n",
    "\n",
    "#### Helpful links\n",
    "- Classification metrics with google developers: https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative\n",
    "- classification metrics: https://www.kdnuggets.com/2020/04/performance-evaluation-metrics-classification.html\n",
    "- pd.get_dummies() and One Hot Encoding: https://queirozf.com/entries/one-hot-encoding-a-feature-on-a-pandas-dataframe-an-example\n",
    "- Differences between Logistic Regression and a Decision Tree: https://www.geeksforgeeks.org/ml-logistic-regression-v-s-decision-tree-classification/\n",
    "- Decision Tree Classifier by Sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "- Understanding classification metrics like Precision, Recall, F-Scores and Confusion matrices: https://nillsf.com/index.php/2020/05/23/confusion-matrix-accuracy-recall-precision-false-positive-rate-and-f-scores-explained/\n",
    "- Understanding the ROC Curve: https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\n",
    "- Use slack for doubts: https://join.slack.com/t/deepconnectai/shared_invite/zt-givlfnf6-~cn3SQ43k0BGDrG9_YOn4g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Uncomment below 2 lines to ignore warnings\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data using wget if running on cloud\n",
    "!wget https://github.com/DeepConnectAI/challenge-week-5/raw/master/data/ozone_levels_train.csv\n",
    "!wget https://github.com/DeepConnectAI/challenge-week-5/raw/master/data/ozone_levels_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and test data\n",
    "train = \n",
    "test  = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore train dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "X_train = \n",
    "X_test  = \n",
    "y_train = \n",
    "y_test  = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shape of X_train, X_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the models\n",
    "# Classifier 1 - Logistic regression\n",
    "clf1 = \n",
    "# Classifier 2 - Decision tree\n",
    "clf2 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train both the models on training dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on testing data\n",
    "y_pred_lr = \n",
    "y_pred_dt = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary building blocks of classification metrics\n",
    "\n",
    "A __TRUE POSITIVE (TP)__ is an outcome where the model correctly predicts the positive class.\n",
    "\n",
    "A __TRUE NEGATIVE (TN)__ is an outcome where the model correctly predicts the negative class.\n",
    "\n",
    "A __FALSE POSITIVE (FP)__ is an outcome where the model incorrectly predicts the positive class.\n",
    "\n",
    "a __FALSE NEGATIVE (FN)__ is an outcome where the model incorrectly predicts the negative class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute primary metrics for logisitc regression\n",
    "# NOTE: All metrics are to be calculated on test dataset\n",
    "# True Positive\n",
    "lr_true_positive = \n",
    "# True Negative\n",
    "lr_true_negative = \n",
    "# False Positive\n",
    "lr_false_positive = \n",
    "# False Negative\n",
    "lr_false_negative = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute primary metrics for decision tree\n",
    "# True Positive\n",
    "dt_true_positive = \n",
    "# True Negative\n",
    "dt_true_negative = \n",
    "# False Positive\n",
    "dt_false_positive = \n",
    "# False Negative\n",
    "dt_false_negative = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "A confusion matrix is visualization technique to summarize the basic performance of a classification algorithm.\n",
    "\n",
    "![Confusion matrix](https://static.packt-cdn.com/products/9781838555078/graphics/C13314_06_05.jpg \"Confusion matric diagram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix, DO NOT EDIT THE CELL\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(25,8))\n",
    "\n",
    "plt.title(\"Confusion matrix for logistic regression\")\n",
    "sns.heatmap(np.array([[lr_true_negative, lr_false_positive],[lr_false_negative, lr_true_positive]]), annot=True, cmap=plt.cm.Blues, fmt='g', ax=axes[0])\n",
    "plt.title(\"Confusion matrix for decision tree\")\n",
    "sns.heatmap(np.array([[dt_true_negative, dt_false_positive],[dt_false_negative, dt_true_positive]]), annot=True, cmap=plt.cm.Blues, fmt='g', ax=axes[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy \n",
    "Classification accuracy is simply the rate of correct classifications\n",
    "$$Accuracy = \\frac{Number \\, of \\, correct \\, predictions}{Total \\, number \\, of \\, predictions}$$\n",
    "<br>\n",
    "$$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification accuracy for logistic regression\n",
    "lr_accuracy = \n",
    "# Classification accuracy for decision tree\n",
    "dt_accuracy = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classificaton accuracy: LR = \" , lr_accuracy)\n",
    "print(\"Classificaton accuracy: DT = \" , dt_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "What proportion of positive identifications was actually correct?\n",
    "$$Precision = \\frac{TP}{TP+FP}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision for logistic regression\n",
    "try:\n",
    "    lr_precision = \n",
    "except:\n",
    "    lr_precision = 0\n",
    "    print(\"If you see this message, it means that the\\ndenominator of precision for logistic regression turned out to be 0 \")\n",
    "# Precision for decision tree\n",
    "try:\n",
    "    dt_precision = \n",
    "except:\n",
    "    dt_precision = 0\n",
    "    print(\"If you see this message, it means that the\\ndenominator of precision for decision tree turned out to be 0 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision: LR = \" , lr_precision)\n",
    "print(\"Precision: DT = \" , dt_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "What proportion of actual positives was identified correctly?\n",
    "$$Recall = \\frac{TP}{TP+FN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall for logistic regression\n",
    "try:\n",
    "    lr_recall = \n",
    "except:\n",
    "    lr_recall = 0\n",
    "    print(\"If you see this message, it means that the\\ndenominator of recall for logistic regression turned out to be 0 \")\n",
    "# Recall for decision tree\n",
    "try:\n",
    "    dt_recall = \n",
    "except:\n",
    "    dt_recall = 0\n",
    "    print(\"If you see this message, it means that the\\ndenominator of recall for decision tree turned out to be 0 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recall: LR = \" , lr_recall)\n",
    "print(\"Recall: DT = \" , dt_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 score\n",
    "The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal.\n",
    "$$ F1 \\, score = \\frac{2* Precision * Recall}{Precision + Recall}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score for logistic regression\n",
    "lr_f1_score = \n",
    "# F1 score for decision tree\n",
    "dt_f1_score = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"F1 score: LR = \" , lr_f1_score)\n",
    "print(\"F1 score: DT = \" , dt_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Under ROC Curve\n",
    "A ROC Curve is a plot of the true positive rate and the false positive rate for a given set of probability predictions at different thresholds used to map the probabilities to class labels. The area under the curve is then the approximate integral under the ROC Curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False positive rate for logisitic regression\n",
    "lr_false_positive_rate = \n",
    "# True positive rate for logisitic regression\n",
    "lr_true_positive_rate = \n",
    "# False positive rate for decision trees\n",
    "dt_false_positive_rate = \n",
    "# True positive rate for decision trees\n",
    "dt_true_positive_rate = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the ROC curve\n",
    "plt.plot()\n",
    "plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
